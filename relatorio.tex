\documentclass[10pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{lmodern}
\usepackage{textcomp} % para símbolos como º
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{float}
\usepackage{microtype}

\lstdefinestyle{CStyle}{
  language=C,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=5pt,
  frame=single,
  breaklines=true,
  showstringspaces=false
}

\title{\textbf{Project Assignment I: All-Pairs Shortest Path (APSP)}\\
\large Repeated Squaring and Fox’s Algorithm using MPI}

\author{Sérgio Cardoso up202107918 \\ Kathleen Soares up201903010}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report presents a parallel implementation of the \emph{All-Pairs Shortest Path} (APSP) problem using the min-plus product and the \emph{Repeated Squaring} method combined with Fox's algorithm on MPI. The program, developed in C, employs a distributed grid of \(Q \times Q\) processes (\(P = Q^2\)). The main design choices, communication strategy, and preliminary performance results demonstrate the scalability and communication trade-offs of the parallel approach.
\end{abstract}

\section{Introduction}
This project computes the shortest distances between all pairs of vertices in a weighted directed graph, represented by an adjacency matrix \(A\). The graph is represented by an adjacency matrix \(A\), where each entry \(A_{ij}\) corresponds to the edge weight or infinity when no connection exists.
 The algorithm applies the min-plus product and the \emph{Repeated Squaring} method, with each multiplication parallelized using Fox’s algorithm on MPI.

\section{Algorithmic Base}

\subsection{Min-Plus Product}
The min-plus product of matrices \(A\) and \(B\) is defined as:
\[
C_{ij} = \min_k (A_{ik} + B_{kj})
\]
This replaces the standard addition by the minimum and multiplication by addition, propagating minimal path distances. In the code, this logic appears in \texttt{local\_minplus\_mm()}, where each process computes its local block of \(C\) from the corresponding blocks of \(A\) and \(B\).

\subsection{Repeated Squaring}
The method applies the min-plus product repeatedly (\(A^2, A^4, A^8, \ldots\)) until \(2^k \ge N-1\). Each iteration calls \texttt{fox\_minplus()}, ensuring all shortest paths are obtained.

\subsection{Fox's Algorithm}
Fox's algorithm is used to multiply blocks of matrices in parallel. The matrix is divided into sub-blocks \((N/Q) \times (N/Q)\), distributed among the processes arranged in a Cartesian grid. Each process performs, in each phase, a broadcast of blocks of \(A\) along the row and a circular rotation of blocks of \(B\) along the column, computing the local part of \(C\) with the min-plus operation.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{matrix_foxImage.png}
  \caption{Example of process grid and communication pattern in Fox's algorithm.}
  \label{fig:fox_algorithm}
\end{figure}

\section{Implementation Details}

\subsection{Main Steps}
\begin{enumerate}
  \item \textbf{Input:}  
  Process 0 reads the adjacency matrix. Zeros outside the diagonal are replaced by \texttt{INF = 1e9} to represent no connection.
  
  \item \textbf{Grid setup:}  
  A 2D Cartesian topology is created. The program checks that \(p = q^2\); otherwise, it prints an error and terminates.
  \item \textbf{Padding:}  
  If \(N\) is not a multiple of \(q\), the matrix is automatically padded to \(N_{pad}\) to match the grid size.
  
  \item \textbf{Distribution:}  
  The (padded) matrix is divided into blocks and assigned to processes according to grid coordinates \((i, j)\).
  \item \textbf{Parallel multiplication:}  
  In each step:
  \begin{itemize}[noitemsep]
    \item block \(A\) is broadcast along the row;
    \item each process computes its local min-plus product with block \(B\);
    \item block \(B\) is rotated vertically using \texttt{MPI\_Sendrecv\_replace}.
  \end{itemize}
\item \textbf{Repeated squaring:}  
  The algorithm repeats min-plus multiplications until \(2^k \ge N-1\).

\item \textbf{Result:}  
  Process (0,0) gathers the final matrix and prints it, replacing infinite values by zeros.
\end{enumerate}

 \subsection*{Input Validation and Padding}
The program validates the number of processes (\(p=q^2\)) and ensures \(N\) is positive and even.  
When \(N\) is not divisible by \(q\), the matrix is padded to size \(N_{pad} = \lceil N/q \rceil \times q\) with INF values and zeros on the diagonal, ensuring compatibility with any process grid.

\section{Implementation Details and Functions}

\subsection{\texttt{grid\_setup}}
Creates an Cartesian topology of processes and the row and column sub-communicators using \texttt{MPI\_Cart\_create} and \texttt{MPI\_Cart\_sub}, 
assigning to each process coordinates \((my\_row, my\_col)\).

\subsection{\texttt{local\_minplus\_mm}}
Executes the min-plus product between two local blocks \(A\) and \(B\), accumulating the result in \(C\):
\[
C[i][j] = \min_k(A[i][k] + B[k][j])
\]
Unnecessary additions involving infinite values are avoided to reduce computational cost.

\subsection{\texttt{fox\_minplus}}
Implements the core of Fox's algorithm.  
At each iteration:
\begin{itemize}
  \item the root process of the row broadcasts its block \(A\);
  \item each process calculates its local contribution via \texttt{local\_minplus\_mm};
  \item the blocks \(B\) are rotated vertically with \texttt{MPI\_Sendrecv\_replace}.
\end{itemize}

\subsection{\texttt{copy\_block\_out} and \texttt{copy\_block\_in}}
Auxiliary functions that copy blocks between the global (padded) matrix and the local submatrices, preserving the original data layout.

\subsection*{Communication Strategy}
Each process broadcasts blocks with \texttt{MPI\_Bcast} along rows and rotates them with \texttt{MPI\_Sendrecv\_replace} along columns.  
Row and column subcommunicators (\texttt{MPI\_Cart\_sub}) ensure synchronization and avoid deadlocks.

\section{Error Handling and Robustness}
The code handles invalid input situations such as:
\begin{itemize}
  \item negative values or \(N = 0\);
  \item non-perfect square number of processes;
 \item incompatible padding configuration.
\end{itemize}
Each memory allocation is checked and aborts execution with an error message in case of failure.

\section{Performance Evaluation}
Execution time and speedup were measured for 1, 4, 9, 16, and 25 processes.  
Only computation and communication times were considered (I/O excluded). Evaluation of performance for N = 300.

\begin{table}[H]
\centering
\caption{CPU user and system times for N = 300 (in seconds, excluding I/O)}
\begin{tabular}{r r r r r}
\toprule
Processes ($P$) & User time (s) & System time (s) & Total (s) & CPU usage (\%) \\
\midrule
1  & 0.73 & 0.11 & 1.234 & 68 \\
4  & 0.91 & 0.43 & 1.093 & 121 \\
9  & 1.21 & 0.91 & 1.417 & 149 \\
16 & 1.50 & 1.25 & 1.738 & 158 \\
25 & 2.30 & 2.64 & 2.883 & 171 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Wall-clock times measured with MPI\_Wtime() for N = 300 (in milliseconds, excluding I/O)}
\begin{tabular}{r r r r}
\toprule
Processes ($P$) & Time ($T_p$) & Speedup ($S_p = T_1 / T_p$) & Efficiency ($E_p = S_p / P$) \\
\midrule
1  & 643.519 & 1.000 & 1.000 \\
4  & 421.535 & 1.527 & 0.382 \\
9  & 454.690 & 1.416 & 0.157 \\
16 & 514.510 & 1.251 & 0.078 \\
25 & 884.413 & 0.727 & 0.029 \\
\bottomrule
\end{tabular}
\end{table}

For N = 6, the execution time increases with the number of processes, since the communication cost outweighs the computational cost. 
For larger matrices, however, parallelism provides noticeable performance improvements.

An initial improvement is observed up to 4 processes (speedup $\approx 1.5$), followed by a performance degradation from 9 processes onward, caused by the increased communication overhead among processes in the 2D grid of the Fox algorithm. 
Efficiency decreases as $P$ increases, which is the expected behavior when the communication cost becomes comparable to or greater than the computational cost.

When comparing the internal \texttt{MPI\_Wtime()} results with the external \texttt{time} command measurements, both indicate consistent wall-clock performance trends.

Overall, parallelism provides limited gains for $N = 300$, while larger matrices would likely scale better.

\section{Challenges and Observations}
The main challenges faced during development were:
\begin{itemize}
  \item ensuring compatibility between matrix size and the number of MPI processes (\(N \bmod Q = 0\));
  \item debugging Fox's communication pattern, particularly the vertical rotation using \texttt{MPI\_Sendrecv\_replace};
  \item implementing and testing automatic padding while keeping the code efficient and readable;
  \item validating numerical accuracy for small test cases and avoiding overflow for large distance values.
\end{itemize}

Overall, the project was valuable to strengthen understanding of distributed programming with MPI, providing practical experience in process synchronization, load balancing, and scalability analysis.
 
As a suggestion, a visual tool to display matrix partitioning and message flow could help future students better understand process decomposition.

\section{Conclusions}
The parallel APSP implementation using Fox’s algorithm with min-plus multiplication proved efficient and scalable.  
Automatic padding and validation improve robustness and flexibility.  
This project consolidated concepts of distributed computation, communication patterns, and performance analysis in MPI.

\begin{thebibliography}{9}
\bibitem{cormen2009introduction}
Cormen, T. H., Leiserson, C. E., Rivest, R. L., \& Stein, C. (2009).
\textit{Introduction to Algorithms} (3rd ed.).
MIT Press.

\bibitem{fox1987}
Fox, G. C., Otto, S. W., \& Hey, A. J. (1987).
\textit{Matrix algorithms on a hypercube I: Matrix multiplication}.
Parallel Computing, 4(1), 17–31.

\bibitem{pacheco1998}
Pacheco, P. S. (1998).
\textit{A User's Guide to MPI}.
San Francisco: Department of Mathematics, University of San Francisco.
  \end{thebibliography}

\vspace{0.3cm}
  
\textit{Note: The complete source code (\texttt{fox.c}) is attached in the ZIP file submitted along with this report.}

\end{document}